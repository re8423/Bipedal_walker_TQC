{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!apt update\n",
    "!apt install xvfb -y\n",
    "!pip install 'swig'\n",
    "!pip install 'pyglet==1.5.27'\n",
    "!pip install 'gym[box2d]==0.20.0'\n",
    "!pip install 'pyvirtualdisplay==3.0'\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Linear\n",
    "from torch.distributions import Distribution, Normal\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import relu, logsigmoid\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import copy\n",
    "from collections import deque\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as disp\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "display = Display(visible=0,size=(600,600))\n",
    "display.start()\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "plot_interval = 10 # update the plot every N episodes\n",
    "video_every = 100 # videos can take a very long time to render so only do it every N episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quantile huber loss function - code sampled from https://github.com/SamsungLabs/tqc_pytorch\n",
    "def quantile_huber_loss_f(quantiles, samples):\n",
    "    pairwise_delta = samples[:, None, None, :] - quantiles[:, :, :, None]  # batch x nets x quantiles x samples\n",
    "    abs_pairwise_delta = torch.abs(pairwise_delta)\n",
    "    huber_loss = torch.where(abs_pairwise_delta > 1,\n",
    "                             abs_pairwise_delta - 0.5,\n",
    "                             pairwise_delta ** 2 * 0.5)\n",
    "\n",
    "    n_quantiles = quantiles.shape[2]\n",
    "    tau = torch.arange(n_quantiles, device=device).float() / n_quantiles + 1 / 2 / n_quantiles\n",
    "    loss = (torch.abs(tau[None, None, :, None] - (pairwise_delta < 0).float()) * huber_loss).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Approximator + Actor networks - code sampled from https://github.com/SamsungLabs/tqc_pytorch\n",
    "\n",
    "LOG_STD_MIN_MAX = (-20, 2)\n",
    "class RescaleAction(gym.ActionWrapper):\n",
    "    def __init__(self, env, a, b):\n",
    "        assert isinstance(env.action_space, spaces.Box), (\n",
    "            \"expected Box action space, got {}\".format(type(env.action_space)))\n",
    "        assert np.less_equal(a, b).all(), (a, b)\n",
    "        super(RescaleAction, self).__init__(env)\n",
    "        self.a = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + a\n",
    "        self.b = np.zeros(env.action_space.shape, dtype=env.action_space.dtype) + b\n",
    "        self.action_space = spaces.Box(low=a, high=b, shape=env.action_space.shape, dtype=env.action_space.dtype)\n",
    "\n",
    "    def action(self, action):\n",
    "        assert np.all(np.greater_equal(action, self.a)), (action, self.a)\n",
    "        assert np.all(np.less_equal(action, self.b)), (action, self.b)\n",
    "        low = self.env.action_space.low\n",
    "        high = self.env.action_space.high\n",
    "        action = low + (high - low)*((action - self.a)/(self.b - self.a))\n",
    "        action = np.clip(action, low, high)\n",
    "        return action\n",
    "\n",
    "\n",
    "class Mlp(Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_size,\n",
    "            hidden_sizes,\n",
    "            output_size\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # TODO: initialization\n",
    "        self.fcs = []\n",
    "        in_size = input_size\n",
    "        for i, next_size in enumerate(hidden_sizes):\n",
    "            fc = Linear(in_size, next_size)\n",
    "            self.add_module(f'fc{i}', fc)\n",
    "            self.fcs.append(fc)\n",
    "            in_size = next_size\n",
    "        self.last_fc = Linear(in_size, output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h = input\n",
    "        for fc in self.fcs:\n",
    "            h = relu(fc(h))\n",
    "        output = self.last_fc(h)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ReplayBuffer(object):\n",
    "    def __init__(self, state_dim, action_dim, max_size=int(1e6)):\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "        self.size = 0\n",
    "\n",
    "        self.transition_names = ('state', 'action', 'next_state', 'reward', 'not_done')\n",
    "        sizes = (state_dim, action_dim, state_dim, 1, 1)\n",
    "        for name, size in zip(self.transition_names, sizes):\n",
    "            setattr(self, name, np.empty((max_size, size)))\n",
    "\n",
    "    def add(self, state, action, next_state, reward, done):\n",
    "        values = (state, action, next_state, reward, 1. - done)\n",
    "        for name, value in zip(self.transition_names, values):\n",
    "            getattr(self, name)[self.ptr] = value\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, self.size, size=batch_size)\n",
    "        names = self.transition_names\n",
    "        return (torch.FloatTensor(getattr(self, name)[ind]).to(device) for name in names)\n",
    "\n",
    "\n",
    "class Critic(Module):\n",
    "    def __init__(self, state_dim, action_dim, n_quantiles, n_nets):\n",
    "        super().__init__()\n",
    "        self.nets = []\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.n_nets = n_nets\n",
    "        for i in range(n_nets):\n",
    "            net = Mlp(state_dim + action_dim, [512, 512, 512], n_quantiles)\n",
    "            self.add_module(f'qf{i}', net)\n",
    "            self.nets.append(net)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        sa = torch.cat((state, action), dim=1)\n",
    "        quantiles = torch.stack(tuple(net(sa) for net in self.nets), dim=1)\n",
    "        return quantiles\n",
    "\n",
    "\n",
    "class Actor(Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.action_dim = action_dim\n",
    "        self.net = Mlp(state_dim, [256, 256], 2 * action_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mean, log_std = self.net(obs).split([self.action_dim, self.action_dim], dim=1)\n",
    "        log_std = log_std.clamp(*LOG_STD_MIN_MAX)\n",
    "\n",
    "        if self.training:\n",
    "            std = torch.exp(log_std)\n",
    "            tanh_normal = TanhNormal(mean, std)\n",
    "            action, pre_tanh = tanh_normal.rsample()\n",
    "            log_prob = tanh_normal.log_prob(pre_tanh)\n",
    "            log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        else:  # deterministic eval without log_prob computation\n",
    "            action = torch.tanh(mean)\n",
    "            log_prob = None\n",
    "        return action, log_prob\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        obs = torch.FloatTensor(obs).to(device)[None, :]\n",
    "        action, _ = self.forward(obs)\n",
    "        action = action[0].cpu().detach().numpy()\n",
    "        return action\n",
    "\n",
    "\n",
    "class TanhNormal(Distribution):\n",
    "    def __init__(self, normal_mean, normal_std):\n",
    "        super().__init__()\n",
    "        self.normal_mean = normal_mean\n",
    "        self.normal_std = normal_std\n",
    "        self.standard_normal = Normal(torch.zeros_like(self.normal_mean, device=device),\n",
    "                                      torch.ones_like(self.normal_std, device=device))\n",
    "        self.normal = Normal(normal_mean, normal_std)\n",
    "\n",
    "    def log_prob(self, pre_tanh):\n",
    "        log_det = 2 * np.log(2) + logsigmoid(2 * pre_tanh) + logsigmoid(-2 * pre_tanh)\n",
    "        result = self.normal.log_prob(pre_tanh) - log_det\n",
    "        return result\n",
    "\n",
    "    def rsample(self):\n",
    "        pretanh = self.normal_mean + self.normal_std * self.standard_normal.sample()\n",
    "        return torch.tanh(pretanh), pretanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"BipedalWalker-v3\")\n",
    "eval_env = gym.make(\"BipedalWalker-v3\")\n",
    "\n",
    "env = gym.wrappers.Monitor(env, \"./video\", video_callable=lambda ep_id: ep_id%100 == 0, force=True)\n",
    "\n",
    "env = RescaleAction(env, -1., 1.)\n",
    "eval_env = RescaleAction(eval_env, -1., 1.)\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "log_f = open(\"agent-log.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agent class, modified from https://github.com/SamsungLabs/tqc_pytorch\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        actor,\n",
    "        critic,\n",
    "        critic_target,\n",
    "        discount,\n",
    "        tau,\n",
    "        top_quantiles_to_drop,\n",
    "        target_entropy,\n",
    "    ):\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.critic_target = critic_target\n",
    "        self.log_alpha = torch.zeros((1,), requires_grad=True, device=device)\n",
    "\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=3e-4)\n",
    "\n",
    "        self.discount = discount\n",
    "        self.tau = tau\n",
    "        self.top_quantiles_to_drop = top_quantiles_to_drop\n",
    "        self.target_entropy = target_entropy\n",
    "\n",
    "        self.quantiles_total = critic.n_quantiles * critic.n_nets\n",
    "\n",
    "        self.total_it = 0\n",
    "\n",
    "    def train(self, replay_buffer, batch_size=256):\n",
    "        state, action, next_state, reward, not_done = replay_buffer.sample(batch_size)\n",
    "    \n",
    "        alpha = torch.exp(self.log_alpha)\n",
    "\n",
    "        # --- Q loss ---\n",
    "        with torch.no_grad():\n",
    "            # get policy action\n",
    "            new_next_action, next_log_pi = self.actor(next_state)\n",
    "\n",
    "            # compute and cut quantiles at the next state\n",
    "            next_z = self.critic_target(next_state, new_next_action)  # batch x nets x quantiles\n",
    "            sorted_z, _ = torch.sort(next_z.reshape(batch_size, -1))\n",
    "            sorted_z_part = sorted_z[:, :self.quantiles_total-self.top_quantiles_to_drop]\n",
    "\n",
    "            # compute target\n",
    "            target = reward + not_done * self.discount * (sorted_z_part - alpha * next_log_pi)\n",
    "\n",
    "        cur_z = self.critic(state, action)\n",
    "        critic_loss = quantile_huber_loss_f(cur_z, target)\n",
    "\n",
    "        # --- Policy and alpha loss ---\n",
    "        new_action, log_pi = self.actor(state)\n",
    "        alpha_loss = -self.log_alpha * (log_pi + self.target_entropy).detach().mean()\n",
    "        \n",
    "        actor_loss = (alpha * log_pi - self.critic(state, new_action).mean(2).mean(1, keepdim=True)).mean()\n",
    "\n",
    "        # --- Update ---\n",
    "\n",
    "        for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "        self.total_it += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training loop - modified from https://github.com/SamsungLabs/tqc_pytorch\n",
    "\n",
    "EPISODE_LENGTH = 2000\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim)\n",
    "actor = Actor(state_dim, action_dim).to(device)\n",
    "critic = Critic(state_dim, action_dim, 25, 5).to(device)\n",
    "critic_target = copy.deepcopy(critic)\n",
    "\n",
    "top_quantiles_to_drop = 2 * 5\n",
    "\n",
    "trainer = Trainer(actor=actor,\n",
    "                  critic=critic,\n",
    "                  critic_target=critic_target,\n",
    "                  top_quantiles_to_drop=top_quantiles_to_drop,\n",
    "                  discount=0.99,\n",
    "                  tau=0.005,\n",
    "                  target_entropy=-np.prod(env.action_space.shape).item())\n",
    "\n",
    "evaluations = []\n",
    "state, done = env.reset(), False\n",
    "episode_return = 0\n",
    "episode_timesteps = 0\n",
    "episode_num = 0\n",
    "\n",
    "actor.train()\n",
    "\n",
    "for t in range(int(1200000)):\n",
    "        \n",
    "        action = actor.select_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode_timesteps += 1\n",
    "        episode_return += reward\n",
    "            \n",
    "        #Augment rewards for hardcore\n",
    "        # if reward == -100.0:\n",
    "        #     reward = -5.0\n",
    "        # else:\n",
    "        #     reward *= 5.0\n",
    "\n",
    "        replay_buffer.add(state, action, next_state, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "        \n",
    "        # Train agent after collecting sufficient data\n",
    "        if t >= 256:\n",
    "            trainer.train(replay_buffer, 256)\n",
    "\n",
    "\n",
    "        if done or episode_timesteps >= EPISODE_LENGTH:\n",
    "            # +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True\n",
    "            print(f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_return:.3f}\")\n",
    "            # Reset environment\n",
    "            state, done = env.reset(), False\n",
    "            log_f.write('episode: {}, reward: {}\\n'.format(episode_num, episode_return))\n",
    "            log_f.flush()\n",
    "            episode_return = 0\n",
    "            episode_timesteps = 0\n",
    "            episode_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd425e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open log file\n",
    "with open(\"agent-log.txt\", \"r\") as file: \n",
    "    log = file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract rewards from log\n",
    "reward_list = []\n",
    "for line in log: \n",
    "    reward_list.append(float(line.strip().split()[-1]))\n",
    "\n",
    "#average and std for past 10 eps\n",
    "averages_10 = []\n",
    "stds_10 = []  \n",
    "\n",
    "for i in range(10 ,len(reward_list)): \n",
    "    averages_10.append(np.array(reward_list[i-10:i]).mean())\n",
    "    stds_10.append(np.array(reward_list[i-10:i]).std())\n",
    "    \n",
    "averages_10 = np.array(averages_10)\n",
    "stds = np.array(stds_10)\n",
    "\n",
    "#average and std for past 100 eps\n",
    "averages_100 = []\n",
    "stds_100 = []  \n",
    "\n",
    "for i in range(100 ,len(reward_list)): \n",
    "    averages_100.append(np.array(reward_list[i-100:i]).mean())\n",
    "    stds_100.append(np.array(reward_list[i-100:i]).std())\n",
    "\n",
    "averages_100 = np.array(averages_100)\n",
    "stds_100 = np.array(stds_100)\n",
    "\n",
    "\n",
    "#Graph\n",
    "plt.plot(list(range(10, len(reward_list))), averages_10, label=f'average over {10} eps', color='mediumblue')\n",
    "plt.plot(list(range(100, len(reward_list))), averages_100, label=f'average over {100} eps', color='fuchsia')\n",
    "plt.fill_between(list(range(100, len(reward_list))), averages_100-stds_100, averages_100+stds_100, alpha=0.2, color='tab:grey')\n",
    "    \n",
    "plt.legend(loc = 'lower right')\n",
    "plt.xlabel('Episode number')\n",
    "plt.ylabel('Episode reward')\n",
    "plt.axhline(y=300, color='red')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ncckernel",
   "language": "python",
   "name": "ncckernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
